{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %load_ext jupyternotify\n",
    "# %autonotify -a 30\n",
    "\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "import keras.layers\n",
    "import keras.applications\n",
    "import keras.backend\n",
    "import keras.preprocessing.image\n",
    "import keras.utils\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shutil\n",
    "\n",
    "from IPython.display import clear_output as clr \n",
    "# configurations\n",
    "\n",
    "## seeding\n",
    "os.environ['PYTHONHASHSEED'] = '3'\n",
    "np.random.seed(3)\n",
    "random.seed(3)\n",
    "tf.set_random_seed(3)\n",
    "\n",
    "## which gpu to use\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "## memory allocation\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "session = tf.Session(config=config)\n",
    "keras.backend.set_session(session)\n",
    "\n",
    "## data directory for CUB200 root\n",
    "PATH_DATA_ROOT_CUB200 = \"D:\\project\\commons\\CUB_200_2011\"\n",
    "\n",
    "## network configurations\n",
    "### number of output classes, 200 for CUB200\n",
    "NO_CLASS = 200\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(x, size_target=None, flg_keep_aspect=False, rate_scale=1.0, flg_random_scale=False):\n",
    "\n",
    "    # convert to numpy array\n",
    "    if not isinstance(x, np.ndarray):\n",
    "        img = np.asarray(x)\n",
    "    else:\n",
    "        img = x\n",
    "\n",
    "    # calculate resize coefficients\n",
    "    if len(img.shape) == 4:\n",
    "        _o, size_height_img, size_width_img, _c , = img.shape\n",
    "        img = img[0]\n",
    "    elif len(img.shape) == 3:\n",
    "        size_height_img, size_width_img, _c , = img.shape\n",
    "\n",
    "    if len(size_target) == 1:\n",
    "        size_heigth_target = size_target\n",
    "        size_width_target = size_target\n",
    "    if len(size_target) == 2:\n",
    "        size_heigth_target = size_target[0]\n",
    "        size_width_target = size_target[1]\n",
    "    if size_target == None:\n",
    "        size_heigth_target = size_height_img * rate_scale \n",
    "        size_width_target = size_width_img * rate_scale \n",
    "\n",
    "    coef_height = 1\n",
    "    coef_width = 1\n",
    "    if size_height_img < size_heigth_target :\n",
    "        coef_height = size_heigth_target / size_height_img\n",
    "    if size_width_img < size_width_target :\n",
    "        coef_width = size_width_target / size_width_img\n",
    "\n",
    "    # calculate coeffieient to match small size to target size\n",
    "    ## scale coefficient if specified\n",
    "    low_scale = rate_scale\n",
    "    if flg_random_scale:\n",
    "        low_scale = 1.0\n",
    "    coef_max = max(coef_height, coef_width) * np.random.uniform(low=low_scale, high=rate_scale)\n",
    "\n",
    "    # resize image\n",
    "    size_height_resize = math.ceil(size_height_img*coef_max)\n",
    "    size_width_resize = math.ceil(size_width_img*coef_max)\n",
    "\n",
    "    # method_interpolation = cv2.INTER_LINEAR\n",
    "    method_interpolation = cv2.INTER_CUBIC\n",
    "    # method_interpolation = cv2.INTER_NEAREST\n",
    "\n",
    "    if flg_keep_aspect:\n",
    "        img_resized = cv2.resize(\n",
    "                            img\n",
    "                            , dsize=(size_width_resize, size_height_resize)\n",
    "                            , interpolation=method_interpolation\n",
    "                        )\n",
    "    else:\n",
    "        img_resized = cv2.resize(\n",
    "                            img\n",
    "                            , dsize=(\n",
    "                                int(size_width_target*np.random.uniform(low=low_scale, high=rate_scale))\n",
    "                                ,int(size_heigth_target*np.random.uniform(low=low_scale, high=rate_scale))\n",
    "                            )\n",
    "                            , interpolation=method_interpolation\n",
    "                        )\n",
    "    return img_resized\n",
    "\n",
    "def resize_images(images, **kwargs):\n",
    "    max_images = len(images)\n",
    "    for i in range(max_images):\n",
    "        images[i] = resize_image(images[i], **kwargs)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"images.npy\")\n",
    "labels = np.load(\"combined_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_resized = resize_images(data, size_target=(224,224), flg_keep_aspect=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2423, 224, 224, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_resized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import glorot_normal\n",
    "\n",
    "def outer_product(x):\n",
    "    \"\"\"\n",
    "    calculate outer-products of 2 tensors\n",
    "\n",
    "        args \n",
    "            x\n",
    "                list of 2 tensors\n",
    "                , assuming each of which has shape = (size_minibatch, total_pixels, size_filter)\n",
    "    \"\"\"\n",
    "    return keras.backend.batch_dot(\n",
    "                x[0]\n",
    "                , x[1]\n",
    "                , axes=[1,1]\n",
    "            ) / x[0].get_shape().as_list()[1] \n",
    "\n",
    "def signed_sqrt(x):\n",
    "    \"\"\"\n",
    "    calculate element-wise signed square root\n",
    "\n",
    "        args\n",
    "            x\n",
    "                a tensor\n",
    "    \"\"\"\n",
    "    return keras.backend.sign(x) * keras.backend.sqrt(keras.backend.abs(x) + 1e-9)\n",
    "\n",
    "def L2_norm(x, axis=-1):\n",
    "    \"\"\"\n",
    "    calculate L2-norm\n",
    "\n",
    "        args \n",
    "            x\n",
    "                a tensor\n",
    "    \"\"\"\n",
    "    return keras.backend.l2_normalize(x, axis=axis)\n",
    "\n",
    "\n",
    "def build_model(\n",
    "    size_heigth=224\n",
    "    ,size_width=224\n",
    "    ,no_class=36\n",
    "    ,no_last_layer_backbone=13\n",
    "    \n",
    "    ,name_optimizer=\"sgd\"\n",
    "    ,rate_learning=1.0\n",
    "    ,rate_decay_learning=0.0\n",
    "    ,rate_decay_weight=0.0\n",
    "    \n",
    "    ,name_initializer=\"glorot_normal\"\n",
    "    ,name_activation_logits=\"softmax\"\n",
    "    ,name_loss=\"categorical_crossentropy\"\n",
    "\n",
    "    ,flg_debug=False\n",
    "    ,**kwargs\n",
    "):\n",
    "    \n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    print(\"-------------------------------\")\n",
    "    print(\"parameters:\")\n",
    "    for key, val in locals().items():\n",
    "        if not val == None and not key == \"kwargs\":\n",
    "            print(\"\\t\", key, \"=\",  val)\n",
    "    print(\"-------------------------------\")\n",
    "    \n",
    "    ### \n",
    "    ### load pre-trained model\n",
    "    ###\n",
    "    tensor_input = keras.layers.Input(shape=[size_heigth,size_width,3])\n",
    "    model_detector = keras.applications.vgg16.VGG16(\n",
    "                            input_tensor=tensor_input\n",
    "                            , include_top=False\n",
    "                            , weights='imagenet'\n",
    "                        )\n",
    "    \n",
    "\n",
    "    ### \n",
    "    ### bi-linear pooling\n",
    "    ###\n",
    "\n",
    "    # extract features from detector\n",
    "    x_detector = model_detector.layers[no_last_layer_backbone].output\n",
    "    shape_detector = model_detector.layers[no_last_layer_backbone].output_shape\n",
    "    if flg_debug:\n",
    "        print(\"shape_detector : {}\".format(shape_detector))\n",
    "\n",
    "    # extract features from extractor , same with detector for symmetry DxD model\n",
    "    shape_extractor = shape_detector\n",
    "    x_extractor = x_detector\n",
    "    if flg_debug:\n",
    "        print(\"shape_extractor : {}\".format(shape_extractor))\n",
    "        \n",
    "    \n",
    "    # rehape to (minibatch_size, total_pixels, filter_size)\n",
    "    x_detector = keras.layers.Reshape(\n",
    "            [\n",
    "                shape_detector[1] * shape_detector[2] , shape_detector[-1]\n",
    "            ]\n",
    "        )(x_detector)\n",
    "    if flg_debug:\n",
    "        print(\"x_detector shape after rehsape ops : {}\".format(x_detector.shape))\n",
    "        \n",
    "    x_extractor = keras.layers.Reshape(\n",
    "            [\n",
    "                shape_extractor[1] * shape_extractor[2] , shape_extractor[-1]\n",
    "            ]\n",
    "        )(x_extractor)\n",
    "    if flg_debug:\n",
    "        print(\"x_extractor shape after rehsape ops : {}\".format(x_extractor.shape))\n",
    "        \n",
    "        \n",
    "    # outer products of features, output shape=(minibatch_size, filter_size_detector*filter_size_extractor)\n",
    "    x = keras.layers.Lambda(outer_product)(\n",
    "        [x_detector, x_extractor]\n",
    "    )\n",
    "    if flg_debug:\n",
    "        print(\"x shape after outer products ops : {}\".format(x.shape))\n",
    "        \n",
    "        \n",
    "    # rehape to (minibatch_size, filter_size_detector*filter_size_extractor)\n",
    "    x = keras.layers.Reshape([shape_detector[-1]*shape_extractor[-1]])(x)\n",
    "    if flg_debug:\n",
    "        print(\"x shape after rehsape ops : {}\".format(x.shape))\n",
    "        \n",
    "        \n",
    "    # signed square-root \n",
    "    x = keras.layers.Lambda(signed_sqrt)(x)\n",
    "    if flg_debug:\n",
    "        print(\"x shape after signed-square-root ops : {}\".format(x.shape))\n",
    "        \n",
    "    # L2 normalization\n",
    "    x = keras.layers.Lambda(L2_norm)(x)\n",
    "    if flg_debug:\n",
    "        print(\"x shape after L2-Normalization ops : {}\".format(x.shape))\n",
    "\n",
    "\n",
    "\n",
    "    ### \n",
    "    ### attach FC-Layer\n",
    "    ###\n",
    "\n",
    "    if name_initializer != None:\n",
    "            name_initializer = eval(name_initializer+\"()\")\n",
    "            \n",
    "    x = keras.layers.Dense(\n",
    "            units=no_class\n",
    "            ,kernel_regularizer=keras.regularizers.l2(rate_decay_weight)\n",
    "            ,kernel_initializer=name_initializer\n",
    "        )(x)\n",
    "    if flg_debug:\n",
    "        print(\"x shape after Dense ops : {}\".format(x.shape))\n",
    "    tensor_prediction = keras.layers.Activation(name_activation_logits)(x)\n",
    "    if flg_debug:\n",
    "        print(\"prediction shape : {}\".format(tensor_prediction.shape))\n",
    "\n",
    "        \n",
    "\n",
    "    ### \n",
    "    ### compile model\n",
    "    ###\n",
    "    model_bilinear = keras.models.Model(\n",
    "                        inputs=[tensor_input]\n",
    "                        , outputs=[tensor_prediction]\n",
    "                    )\n",
    "    \n",
    "    \n",
    "    # fix pre-trained weights\n",
    "    for layer in model_detector.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "        \n",
    "    # define optimizers\n",
    "    opt_adam = keras.optimizers.adam(\n",
    "                    lr=rate_learning\n",
    "                    , decay=rate_decay_learning\n",
    "                )\n",
    "    opt_rms = keras.optimizers.RMSprop(\n",
    "                    lr=rate_learning\n",
    "                    , decay=rate_decay_learning\n",
    "                )\n",
    "    opt_sgd = keras.optimizers.SGD(\n",
    "                    lr=rate_learning\n",
    "                    , decay=rate_decay_learning\n",
    "                    , momentum=0.9\n",
    "                    , nesterov=False\n",
    "                )\n",
    "    optimizers ={\n",
    "        \"adam\":opt_adam\n",
    "        ,\"rmsprop\":opt_rms\n",
    "        ,\"sgd\":opt_sgd\n",
    "    }\n",
    "    \n",
    "    model_bilinear.compile(\n",
    "        loss=name_loss\n",
    "        , optimizer=optimizers[name_optimizer]\n",
    "        , metrics=[\"categorical_accuracy\"]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    if flg_debug:\n",
    "        model_bilinear.summary()\n",
    "    \n",
    "    return model_bilinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "parameters:\n",
      "\t flg_debug = True\n",
      "\t name_loss = categorical_crossentropy\n",
      "\t name_activation_logits = softmax\n",
      "\t name_initializer = glorot_normal\n",
      "\t rate_decay_weight = 1e-08\n",
      "\t rate_decay_learning = 0.0\n",
      "\t rate_learning = 1.0\n",
      "\t name_optimizer = sgd\n",
      "\t no_last_layer_backbone = 13\n",
      "\t no_class = 36\n",
      "\t size_width = 224\n",
      "\t size_heigth = 224\n",
      "-------------------------------\n",
      "shape_detector : (None, 28, 28, 512)\n",
      "shape_extractor : (None, 28, 28, 512)\n",
      "x_detector shape after rehsape ops : (?, 784, 512)\n",
      "x_extractor shape after rehsape ops : (?, 784, 512)\n",
      "x shape after outer products ops : (?, 512, 512)\n",
      "x shape after rehsape ops : (?, 262144)\n",
      "x shape after signed-square-root ops : (?, 262144)\n",
      "x shape after L2-Normalization ops : (?, 262144)\n",
      "x shape after Dense ops : (?, 36)\n",
      "prediction shape : (?, 36)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 224, 224, 64) 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 224, 224, 64) 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 112, 112, 64) 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 112, 112, 128 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 112, 112, 128 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 56, 56, 128)  0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 56, 56, 256)  295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 28, 28, 512)  1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 784, 512)     0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 784, 512)     0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 512, 512)     0           reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 262144)       0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 262144)       0           reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 262144)       0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 36)           9437220     lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 36)           0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 17,072,484\n",
      "Trainable params: 9,437,220\n",
      "Non-trainable params: 7,635,264\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "            # number of output classes, 200 for CUB200\n",
    "            no_class = 36\n",
    "\n",
    "            # pretrained model specification, using VGG16\n",
    "            # \"block5_conv3 \"\n",
    "            ,no_last_layer_backbone = 13\n",
    "    \n",
    "            # training parametes\n",
    "            ,rate_learning=1.0\n",
    "            ,rate_decay_weight=1e-8\n",
    "    \n",
    "            ,flg_debug=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        model=None\n",
    "        ,name_model=\"BCNN_keras\"\n",
    "        ,X_train = None\n",
    "        ,Y_train = None\n",
    "        ,X_test = None\n",
    "        ,Y_test = None\n",
    "        ,max_epoch=50\n",
    "    ):\n",
    "    \n",
    "    path_model = \"./model/{}/\".format(name_model)\n",
    "    if not os.path.exists(path_model):\n",
    "        os.mkdir(path_model)\n",
    "        \n",
    "    now = time.strftime(\"%Y%m%d%H%M%S\", time.localtime())\n",
    "        \n",
    "    # callback setting\n",
    "    callback_logger = keras.callbacks.CSVLogger(\n",
    "                            path_model +  \"log_training_{}.csv\".format(now)\n",
    "                            , separator=','\n",
    "                            , append=False\n",
    "                        )\n",
    "    callack_saver = keras.callbacks.ModelCheckpoint(\n",
    "                        path_model\n",
    "                            + \"E[{epoch:02d}]\"\n",
    "                            + \"_LOS[{val_loss:.3f}]\"\n",
    "                            + \"_ACC[{val_categorical_accuracy:.3f}]\"\n",
    "                            + \".hdf5\" \n",
    "                        , monitor='val_loss'\n",
    "                        , verbose=0\n",
    "                        , mode='auto'\n",
    "                        , period=10\n",
    "                        , save_best_only=True\n",
    "                    )\n",
    "    callback_reducer = keras.callbacks.ReduceLROnPlateau(\n",
    "                                monitor='val_loss'\n",
    "                                , factor=0.5\n",
    "                                , patience=5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
    "                                , min_lr=1e-6\n",
    "                                , min_delta=1e-3\n",
    "                            )\n",
    "    callback_stopper = keras.callbacks.EarlyStopping(\n",
    "                            monitor='val_loss'\n",
    "                            , min_delta=1e-3\n",
    "                            , patience=10\n",
    "                            , verbose=0\n",
    "                            , mode='auto'\n",
    "                        )\n",
    "    list_callback = [\n",
    "        callback_logger\n",
    "        ,callack_saver\n",
    "        ,callback_reducer\n",
    "        ,callback_stopper\n",
    "    ]\n",
    "            \n",
    "    hist = model.fit(\n",
    "                X_train,\n",
    "                Y_train\n",
    "                , epochs=max_epoch\n",
    "                , validation_data=(X_test, Y_test)\n",
    "                ,callbacks=list_callback\n",
    "                ,verbose=1\n",
    "                ,batch_size = 5\n",
    "            )\n",
    "        \n",
    "    model.save_weights(\n",
    "        path_model\n",
    "            + \"E[{}]\".format(len(hist.history['val_loss']))\n",
    "            + \"_LOS[{:.3f}]\".format(hist.history['val_loss'][-1])\n",
    "            + \"_ACC[{:.3f}]\".format(hist.history['val_categorical_accuracy'][-1])\n",
    "            + \".h5\" \n",
    "    )\n",
    "    \n",
    "    return hist                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_labels = keras.utils.to_categorical(labels, num_classes = 36)\n",
    "x_tr, x_ts, y_tr, y_ts = tts(data_resized, oh_labels, test_size = 0.2, stratify = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('bcnnw.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now all layers are trainable\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# change LR\n",
    "opt_sgd = keras.optimizers.SGD(\n",
    "                lr=1e-3\n",
    "                , decay=1e-9\n",
    "                , momentum=0.9\n",
    "                , nesterov=False\n",
    "            )\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\"\n",
    "    , optimizer=opt_sgd\n",
    "    , metrics=[\"categorical_accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist =train_model(model=model\n",
    "#             ,X_train = x_tr\n",
    "#             ,Y_train = y_tr\n",
    "#             ,X_test = x_ts\n",
    "#             ,Y_test = y_ts\n",
    "#             ,max_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describeSURF(image):\n",
    "    surf = cv2.xfeatures2d.SURF_create()\n",
    "    surf.setHessianThreshold(400)\n",
    "    kp, des = surf.detectAndCompute(image,None)\n",
    "    return kp,des\n",
    "\n",
    "def describeSIFT(image):\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    kp, des = sift.detectAndCompute(image,None)\n",
    "    return kp,des\n",
    "\n",
    "def describeORB(image):\n",
    "    orb=cv2.ORB_create()\n",
    "    kp, des=orb.detectAndCompute(image,None)\n",
    "    return kp,des\n",
    "\n",
    "def getDescriptors(images,function):\n",
    "    \n",
    "    descriptors = []\n",
    "    \n",
    "    for im in images:\n",
    "        \n",
    "        kp,des = function(im)\n",
    "        \n",
    "        if des is not None:\n",
    "            descriptors.append(des)\n",
    "            \n",
    "    descriptors = list(itertools.chain.from_iterable(descriptors))\n",
    "    descriptors = np.asarray(descriptors)\n",
    "\n",
    "    return descriptors\n",
    "\n",
    "def get_kMeansModel(X, k):\n",
    "    model = KMeans(n_clusters=k,init='k-means++',tol=0.0001,verbose=1).fit(X)\n",
    "    return model\n",
    "\n",
    "def get_VLAD(X,model):\n",
    "\n",
    "    c_vals = model.predict(X)\n",
    "    centers = model.cluster_centers_\n",
    "    labels = model.labels_\n",
    "    k = model.n_clusters\n",
    "   \n",
    "    m,d = X.shape\n",
    "    V=np.zeros([k,d])\n",
    "    \n",
    "    for i in range(k):\n",
    "        if np.sum(c_vals==i)>0:\n",
    "            V[i]=np.sum(X[c_vals == i,:]-centers[i],axis=0)\n",
    "            \n",
    "\n",
    "    V = V.flatten()\n",
    "    \n",
    "    V = np.sign(V)*np.sqrt(np.abs(V))\n",
    "\n",
    "    # L2 normalization\n",
    "    V = V/np.sqrt(np.dot(V,V))\n",
    "    \n",
    "    \n",
    "    vec = np.zeros(k)\n",
    "    cv = np.unique(c_vals, return_counts =True)\n",
    "    vec[cv[0]] = cv[1]\n",
    "    \n",
    "    vec = vec/np.sqrt(np.dot(vec,vec))\n",
    "    vlad_vec = np.concatenate((V, vec))\n",
    "    \n",
    "    return vlad_vec\n",
    "\n",
    "def getVLAD_Descriptors(images,function,k_model):\n",
    "    descriptors = []\n",
    "\n",
    "    \n",
    "    \n",
    "    for im in images:\n",
    "        \n",
    "        dess = []\n",
    "        kp,des = function(im)\n",
    "        if des is not None:\n",
    "            dess.append(des)\n",
    "        \n",
    "        dess = list(itertools.chain.from_iterable(dess))\n",
    "        dess = np.asarray(dess)\n",
    "        \n",
    "        description = get_VLAD(dess,k_model)\n",
    "        \n",
    "        descriptors.append(description)\n",
    "    \n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"k_model_sift.pkl\", \"rb\") as file:\n",
    "    k_model_sift = pkl.load(file) \n",
    "\n",
    "with open(\"k_model_surf.pkl\", \"rb\") as file:\n",
    "    k_model_surf = pkl.load(file) \n",
    "    \n",
    "with open(\"k_model_orb.pkl\", \"rb\") as file:\n",
    "    k_model_orb = pkl.load(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desc_sift = getVLAD_Descriptors(x_tr, describeSIFT, k_model_sift)\n",
    "# desc_orb = getVLAD_Descriptors(x_tr, describeORB, k_model_orb)\n",
    "# desc_surf = getVLAD_Descriptors(x_tr, describeSURF, k_model_surf)\n",
    "\n",
    "# desc_net_train = np.concatenate((desc_sift, desc_surf, desc_orb), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desc_sift = getVLAD_Descriptors(x_ts, describeSIFT, k_model_sift)\n",
    "# desc_orb = getVLAD_Descriptors(x_ts, describeORB, k_model_orb)\n",
    "# desc_surf = getVLAD_Descriptors(x_ts, describeSURF, k_model_surf)\n",
    "\n",
    "# desc_net_test = np.concatenate((desc_sift, desc_surf, desc_orb), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mod = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_train = model_mod.predict(x_tr)\n",
    "# pred_test = model_mod.predict(x_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier as MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# red = PCA(n_components = 35).fit(desc_net_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1938, 71), (485, 71))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_train_feat = np.concatenate((red.transform(desc_net_train), pred_train), axis = 1)\n",
    "# final_test_feat = np.concatenate((red.transform(desc_net_test), pred_test), axis = 1)\n",
    "# final_train_feat.shape, final_test_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9855670103092784"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c_model_c = MLP(hidden_layer_sizes =(1000))\n",
    "# c_model_c.fit(final_train_feat, np.argmax(y_tr, axis=1))\n",
    "# c_model_c.score(final_test_feat, np.argmax(y_ts, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the files\n",
    "# with open(\"mlp_model.pkl\", 'wb') as file:  \n",
    "#         pkl.dump(c_model_c, file)\n",
    "\n",
    "# with open(\"pca_model.pkl\", 'wb') as file:  \n",
    "#         pkl.dump(red, file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the files\n",
    "with open(\"mlp_model.pkl\", 'rb') as file:  \n",
    "        c_model_c = pkl.load(file)\n",
    "\n",
    "with open(\"pca_model.pkl\", 'rb') as file:  \n",
    "        red = pkl.load(file)\n",
    "\n",
    "model_mod = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_query(x):\n",
    "    N =x.shape[0]\n",
    "    imgra = []\n",
    "\n",
    "    for i in range(N):\n",
    "        imgra.append(resize_image(x[i], size_target=(224,224)))\n",
    "    \n",
    "    imgr = np.array(imgra)\n",
    "    \n",
    "    desc_sift = getVLAD_Descriptors(imgr, describeSIFT, k_model_sift)\n",
    "    desc_orb = getVLAD_Descriptors(imgr, describeORB, k_model_orb)\n",
    "    desc_surf = getVLAD_Descriptors(imgr, describeSURF, k_model_surf)\n",
    "\n",
    "    desc_net = np.concatenate((desc_sift, desc_surf, desc_orb), axis = 1)\n",
    "    \n",
    "    pred = model_mod.predict(imgr)\n",
    "    \n",
    "    final_feat = np.concatenate((red.transform(desc_net), pred), axis = 1)\n",
    "    y_hat = c_model_c.predict(final_feat)\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnames = np.load('bnames.npy')\n",
    "\n",
    "fnames = np.load('fine_labels.npy')\n",
    "\n",
    "X_query = np.load('images.npy')\n",
    "\n",
    "Y_query = np.load('combined_labels.npy')\n",
    "\n",
    "Y_query_b = np.load('broad_labels.npy')\n",
    "\n",
    "bnames[bnames == 'birds_'] = 'birds'\n",
    "bnames[bnames == 'dogs_'] = 'dogs'\n",
    "bnames[bnames == 'flowers_'] = 'flowers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_names = np.load('fine_names2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(imagenames , images):\n",
    "    \n",
    "    query_in = imagenames.copy().astype(str)\n",
    "    query_out = []\n",
    "    preds = solve_query(images)\n",
    "    \n",
    "    for i in range(preds.shape[0]):\n",
    "        pred = preds[i]\n",
    "        bname = np.unique(bnames[Y_query == pred])[0]\n",
    "        fname = np.unique(fine_names[Y_query == pred])[0]\n",
    "        \n",
    "        var  =  str(query_in[i]) + ' ' + str(bname) +' '+str(bname) +'@'+ str(fname)\n",
    "        query_out.append(str(var))\n",
    "    return query_out, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aircrafts aircrafts aircrafts@0']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, preds = get_output(bnames[0:1], X_query[0:1])\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(preds == Y_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_folder = \"data_test/data\"\n",
    "write_folder = \"data_test/preds\"\n",
    "\n",
    "text_file = open(write_folder+'/'+'pred'+\".txt\", \"w\")\n",
    "counter = 0\n",
    "for img_path in sorted(os.listdir(read_folder)):\n",
    "    print(img_path)\n",
    "    img = np.array(cv2.imread(read_folder+'/'+img_path))\n",
    "    a,b,c = img.shape\n",
    "    out, preds = get_output(np.array([img_path]), img.reshape(1,a,b,c))\n",
    "    out_w = out[0]\n",
    "    print(out_w)\n",
    "    text_file.write(out_w+\"\\n\")\n",
    "    print('Remaining', 1212-counter)\n",
    "    counter+=1\n",
    "    clr()\n",
    "\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_labels2 = np.load(\"fine_labels2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_labels2 = np.load(\"combined_labels2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(fnames == fine_labels2)/fnames.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
